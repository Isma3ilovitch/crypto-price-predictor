{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zJWzO0NRy5m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_csv('/content/drive/MyDrive/data.csv')\n",
        "\n",
        "    # Convert 'Timestamp' column from milliseconds to seconds\n",
        "data['datetime'] = pd.to_datetime(data['Timestamp'] // 1000, unit='s')\n",
        "\n",
        "    # Set the 'datetime' column as the index\n",
        "data.set_index('datetime', inplace=True)\n",
        "data.drop(['Volume BTC', 'Timestamp', 'Symbol', 'Date'], axis=1, inplace=True)\n",
        "\n",
        "    # Rename columns to match Backtrader's expectations\n",
        "data.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume USD': 'volume'}, inplace=True)\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://anaconda.org/conda-forge/libta-lib/0.4.0/download/linux-64/libta-lib-0.4.0-h166bdaf_1.tar.bz2'\n",
        "!curl -L $url | tar xj -C /usr/lib/x86_64-linux-gnu/ lib --strip-components=1\n",
        "url = 'https://anaconda.org/conda-forge/ta-lib/0.4.19/download/linux-64/ta-lib-0.4.19-py310hde88566_4.tar.bz2'\n",
        "!curl -L $url | tar xj -C /usr/local/lib/python3.10/dist-packages/ lib/python3.10/site-packages/talib --strip-components=3\n",
        "import talib"
      ],
      "metadata": {
        "id": "9zmq-z-yR6bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, Dropout, MultiHeadAttention, Flatten, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import talib\n",
        "\n",
        "# Directory Setup\n",
        "checkpoint_dir = \"/content/drive/MyDrive/checkpoint20\"\n",
        "saved_model_dir = \"/content/drive/MyDrive/saved_mode20\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(saved_model_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"model_checkpoint.weights.h5\")\n",
        "epoch_file_path = os.path.join(checkpoint_dir, \"last_epoch.txt\")\n",
        "# Function to load the last epoch\n",
        "def load_last_epoch():\n",
        "    if os.path.exists(epoch_file_path):\n",
        "        with open(epoch_file_path, \"r\") as f:\n",
        "            return int(f.read().strip())\n",
        "    return 0  # Return 0 if no epoch file found, i.e., start from scratch\n",
        "\n",
        "# Function to save the current epoch\n",
        "def save_last_epoch(epoch):\n",
        "    with open(epoch_file_path, \"w\") as f:\n",
        "        f.write(str(epoch))\n",
        "\n",
        "# Define 30-minute horizon for prediction\n",
        "horizon = 30\n",
        "sequence_length = 30\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_data(data):\n",
        "    # Add technical indicators\n",
        "    data['EMA_20'] = data['close'].ewm(span=20).mean()\n",
        "    data['ATR'] = (data['high'] - data['low']).rolling(window=14).mean()\n",
        "    data['OBV'] = (np.sign(data['close'].diff()) * data['volume']).cumsum()\n",
        "    data['close'] = data['close']\n",
        "    # 1. RSI\n",
        "    data['RSI'] = talib.RSI(data['close'], timeperiod=14)\n",
        "    # 2. Moving Averages\n",
        "    data['SMA_20'] = talib.SMA(data['close'], timeperiod=20)\n",
        "    data['SMA_50'] = talib.SMA(data['close'], timeperiod=50)\n",
        "    data['EMA_20'] = talib.EMA(data['close'], timeperiod=20)\n",
        "    data['EMA_50'] = talib.EMA(data['close'], timeperiod=50)\n",
        "    # 3. MACD\n",
        "    data['MACD'], data['MACD_signal'], data['MACD_hist'] = talib.MACD(data['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
        "    # 4. Bollinger Bands\n",
        "    data['upper_band'], data['middle_band'], data['lower_band'] = talib.BBANDS(data['close'], timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
        "    # 5. Volume Analysis (On-Balance Volume)\n",
        "    data['OBV'] = talib.OBV(data['close'], data['volume'])\n",
        "    # Drop NaN values\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(data[['close', 'RSI', 'SMA_20', 'SMA_50', 'MACD', 'upper_band',\n",
        "                                             'lower_band', 'EMA_20', 'ATR', 'OBV']])\n",
        "    return scaled_data, scaler\n",
        "\n",
        "def create_sequences(data, sequence_length, horizon):\n",
        "    X, y = [], []\n",
        "    for i in range(sequence_length, len(data) - horizon):\n",
        "        X.append(data[i - sequence_length:i])  # Past 30 minutes as input\n",
        "        y.append(data[i + horizon - 1, 0])    # Closing price 30 minutes ahead as target\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Hybrid Model\n",
        "def build_hybrid_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # CNN Layer\n",
        "    cnn_layer = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "    cnn_flattened = Flatten()(cnn_layer)\n",
        "\n",
        "    # LSTM Layer\n",
        "    lstm_layer = LSTM(units=64, return_sequences=True)(inputs)\n",
        "\n",
        "    # Attention Layer\n",
        "    attention_layer = MultiHeadAttention(num_heads=4, key_dim=32)(lstm_layer, lstm_layer)\n",
        "    attention_flattened = Flatten()(attention_layer)\n",
        "\n",
        "    # Concatenate CNN and Attention outputs\n",
        "    concatenated = Concatenate()([cnn_flattened, attention_flattened])\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    dense_layer = Dense(64, activation='relu')(concatenated)\n",
        "    dropout_layer = Dropout(0.3)(dense_layer)\n",
        "    outputs = Dense(1)(dropout_layer)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Load Dataset\n",
        "scaled_data, scaler = preprocess_data(data)\n",
        "X, y = create_sequences(scaled_data, sequence_length, horizon)\n",
        "\n",
        "# Train-Test Split\n",
        "split_ratio = 0.8\n",
        "train_size = int(len(X) * split_ratio)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Build and Train Model\n",
        "model = build_hybrid_model((X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "# Check for existing checkpoints\n",
        "initial_epoch = load_last_epoch()\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Checkpoint found. Loading weights from {checkpoint_path}...\")\n",
        "    model.load_weights(checkpoint_path)\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "lr_schedule = LearningRateScheduler(lambda epoch: 1e-4 * (0.9 ** epoch))\n",
        "\n",
        "# Model Checkpoint Callback\n",
        "# Model Checkpoint Callback (Updated)\n",
        "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, save_best_only=False, save_freq='epoch')\n",
        "# Custom Callback to Save Epoch after Each Epoch\n",
        "class EpochSaverCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        save_last_epoch(epoch + 1)  # Save the epoch number at the end of the epoch\n",
        "# Train Model\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    initial_epoch=initial_epoch,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[checkpoint_callback, lr_schedule, EpochSaverCallback()]\n",
        ")\n",
        "# Save Model\n",
        "model.save(os.path.join(saved_model_dir, \"model.keras\"))\n",
        "print(f\"Model saved at {saved_model_dir}/model.keras\")\n",
        "\n",
        "# Save the last epoch number\n",
        "with open(epoch_file_path, \"w\") as f:\n",
        "    f.write(str(initial_epoch + 50))  # Saving the epoch number after training is done."
      ],
      "metadata": {
        "id": "Nyv_HYGuR8_t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}